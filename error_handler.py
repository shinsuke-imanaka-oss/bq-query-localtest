# error_handler.py - å®Œå…¨ä¿®æ­£ç‰ˆ
"""
å¼·åŒ–ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ©Ÿèƒ½
- è©³ç´°ãªã‚¨ãƒ©ãƒ¼åˆ†æã¨åˆ†é¡
- å…·ä½“çš„ãªè§£æ±ºç­–ã®æç¤º
- ã‚¨ãƒ©ãƒ¼ã®è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½
- ã‚¨ãƒ©ãƒ¼å±¥æ­´ã®è¿½è·¡
"""

import streamlit as st
import pandas as pd
import re
import traceback
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import json

class EnhancedErrorHandler:
    """å¼·åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨è§£æ±ºç­–ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.error_patterns = {
            # BigQuery SQLã‚¨ãƒ©ãƒ¼
            "syntax_error": {
                "patterns": ["Syntax error", "syntax", "Expected", "Unexpected"],
                "category": "SQLæ§‹æ–‡ã‚¨ãƒ©ãƒ¼",
                "severity": "high",
                "solutions": [
                    "SQLæ§‹æ–‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "ã‚«ãƒ³ãƒã€æ‹¬å¼§ã€ã‚¯ã‚©ãƒ¼ãƒˆã®å¯¾å¿œã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "äºˆç´„èªã®ä½¿ç”¨æ–¹æ³•ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                ]
            },
            
            "table_not_found": {
                "patterns": ["not found", "does not exist", "Table", "Dataset"],
                "category": "ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼",
                "severity": "high",
                "solutions": [
                    "ãƒ†ãƒ¼ãƒ–ãƒ«åã®ã‚¹ãƒšãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                    "ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                ]
            },
            
            "column_not_found": {
                "patterns": ["Unrecognized name", "Column", "field"],
                "category": "åˆ—åã‚¨ãƒ©ãƒ¼",
                "severity": "medium",
                "solutions": [
                    "åˆ—åã®ã‚¹ãƒšãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "åˆ—åã¯å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ã¾ã™",
                    "åˆ©ç”¨å¯èƒ½ãªåˆ—åã‚’DESCRIBEã‚¯ã‚¨ãƒªã§ç¢ºèªã—ã¦ãã ã•ã„"
                ]
            },
            
            "type_mismatch": {
                "patterns": ["type", "cannot be coerced", "CAST", "conversion"],
                "category": "ãƒ‡ãƒ¼ã‚¿å‹ã‚¨ãƒ©ãƒ¼",
                "severity": "medium",
                "solutions": [
                    "CASTé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦é©åˆ‡ãªå‹ã«å¤‰æ›ã—ã¦ãã ã•ã„",
                    "NULLå€¤ã®å‡¦ç†ã‚’è¿½åŠ ã—ã¦ãã ã•ã„",
                    "SAFE_CASTé–¢æ•°ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
                ]
            },
            
            "aggregate_error": {
                "patterns": ["GROUP BY", "aggregate", "must be grouped"],
                "category": "é›†ç´„é–¢æ•°ã‚¨ãƒ©ãƒ¼",
                "severity": "medium",
                "solutions": [
                    "GROUP BYå¥ã«å¿…è¦ãªåˆ—ã‚’è¿½åŠ ã—ã¦ãã ã•ã„",
                    "é›†ç´„é–¢æ•°ã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„",
                    "éé›†ç´„åˆ—ã‚’GROUP BYå¥ã«å«ã‚ã¦ãã ã•ã„"
                ]
            },
            
            "quota_exceeded": {
                "patterns": ["quota", "exceeded", "limit", "timeout"],
                "category": "ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã‚¨ãƒ©ãƒ¼",
                "severity": "high",
                "solutions": [
                    "ã‚¯ã‚¨ãƒªã®ç¯„å›²ã‚’ç‹­ã‚ã¦ãã ã•ã„ï¼ˆæ—¥ä»˜æ¡ä»¶ã®è¿½åŠ ï¼‰",
                    "LIMITå¥ã‚’ä½¿ç”¨ã—ã¦çµæœæ•°ã‚’åˆ¶é™ã—ã¦ãã ã•ã„",
                    "å¿…è¦ãªåˆ—ã®ã¿ã‚’é¸æŠã—ã¦ãã ã•ã„"
                ]
            },
            
            "permission_error": {
                "patterns": ["permission", "access", "denied", "forbidden"],
                "category": "æ¨©é™ã‚¨ãƒ©ãƒ¼",
                "severity": "high",
                "solutions": [
                    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "BigQueryãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "ç®¡ç†è€…ã«æ¨©é™ã®ä»˜ä¸ã‚’ä¾é ¼ã—ã¦ãã ã•ã„"
                ]
            },
            
            "api_error": {
                "patterns": ["API", "authentication", "key", "unauthorized"],
                "category": "APIèªè¨¼ã‚¨ãƒ©ãƒ¼",
                "severity": "high",
                "solutions": [
                    "APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                    "èªè¨¼æƒ…å ±ã®æœ‰åŠ¹æœŸé™ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                ]
            }
        }
    
    def analyze_error(self, error_message: str, context: Dict = None) -> Dict:
        """ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ"""
        if context is None:
            context = {}
            
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒãƒãƒ³ã‚°
        pattern_match = self._match_error_pattern(error_message)
        
        # è‡ªå‹•ä¿®æ­£ææ¡ˆã®ç”Ÿæˆ
        auto_fix_suggestions = self._generate_auto_fix_suggestions(error_message, context)
        
        # ã‚¨ãƒ©ãƒ¼é‡è¦åº¦ã®åˆ¤å®š
        severity = pattern_match["severity"] if pattern_match else "medium"
        
        return {
            "original_message": error_message,
            "category": pattern_match["category"] if pattern_match else "æœªåˆ†é¡ã‚¨ãƒ©ãƒ¼",
            "severity": severity,
            "solutions": pattern_match["solutions"] if pattern_match else ["ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¦ãã ã•ã„"],
            "auto_fix_suggestions": auto_fix_suggestions,
            "context": context,
            "timestamp": datetime.now()
        }
    
    def _match_error_pattern(self, error_message: str) -> Optional[Dict]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒãƒãƒ³ã‚°"""
        error_lower = error_message.lower()
        
        for pattern_name, pattern_info in self.error_patterns.items():
            for pattern in pattern_info["patterns"]:
                if pattern.lower() in error_lower:
                    return pattern_info
        
        return None
    
    def _generate_auto_fix_suggestions(self, error_message: str, context: Dict) -> List[Dict]:
        """è‡ªå‹•ä¿®æ­£ææ¡ˆã®ç”Ÿæˆ"""
        suggestions = []
        
        # SQLæ–‡ã®å–å¾—
        sql = context.get("sql", "")
        user_input = context.get("user_input", "")
        
        # ä¸€èˆ¬çš„ãªä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³
        if "syntax error" in error_message.lower():
            suggestions.extend(self._suggest_syntax_fixes(sql))
        
        if "not found" in error_message.lower() and "table" in error_message.lower():
            suggestions.extend(self._suggest_table_fixes(sql))
        
        if "unrecognized name" in error_message.lower():
            suggestions.extend(self._suggest_column_fixes(sql, error_message))
        
        if "group by" in error_message.lower():
            suggestions.extend(self._suggest_groupby_fixes(sql))
        
        if "cast" in error_message.lower() or "type" in error_message.lower():
            suggestions.extend(self._suggest_type_fixes(sql))
        
        return suggestions
    
    def _suggest_syntax_fixes(self, sql: str) -> List[Dict]:
        """æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ææ¡ˆ"""
        suggestions = []
        
        # ä¸€èˆ¬çš„ãªæ§‹æ–‡å•é¡Œã®ãƒã‚§ãƒƒã‚¯
        issues = []
        
        # ã‚«ãƒ³ãƒã®å•é¡Œ
        if sql.count("SELECT") > 0:
            # SELECTå¥ã§ã®æœ«å°¾ã‚«ãƒ³ãƒãƒã‚§ãƒƒã‚¯
            select_match = re.search(r'SELECT\s+.*?FROM', sql, re.IGNORECASE | re.DOTALL)
            if select_match:
                select_part = select_match.group()
                if re.search(r',\s*FROM', select_part, re.IGNORECASE):
                    issues.append("SELECTå¥ã®æœ«å°¾ã«ä¸è¦ãªã‚«ãƒ³ãƒãŒã‚ã‚Šã¾ã™")
        
        # æ‹¬å¼§ã®å¯¾å¿œãƒã‚§ãƒƒã‚¯
        open_parens = sql.count('(')
        close_parens = sql.count(')')
        if open_parens != close_parens:
            issues.append(f"æ‹¬å¼§ã®å¯¾å¿œãŒå–ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆé–‹æ‹¬å¼§: {open_parens}, é–‰æ‹¬å¼§: {close_parens}ï¼‰")
        
        # ã‚¯ã‚©ãƒ¼ãƒˆã®å¯¾å¿œãƒã‚§ãƒƒã‚¯
        single_quotes = sql.count("'")
        if single_quotes % 2 != 0:
            issues.append("ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã®å¯¾å¿œãŒå–ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ä¿®æ­£ææ¡ˆã®ç”Ÿæˆ
        for issue in issues:
            suggestions.append({
                "type": "syntax_fix",
                "description": issue,
                "priority": "high",
                "auto_fixable": False
            })
        
        return suggestions
    
    def _suggest_table_fixes(self, sql: str) -> List[Dict]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«é–¢é€£ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ææ¡ˆ"""
        suggestions = []
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        table_patterns = re.findall(r'FROM\s+`?([^`\s]+)`?', sql, re.IGNORECASE)
        
        for table in table_patterns:
            # æ­£ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«åã®æ¨æ¸¬
            if "." not in table:
                suggestions.append({
                    "type": "table_fix",
                    "description": f"ãƒ†ãƒ¼ãƒ–ãƒ«å '{table}' ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’è¿½åŠ ã—ã¦ãã ã•ã„",
                    "suggestion": f"`project_id.dataset_id.{table}`",
                    "priority": "high",
                    "auto_fixable": True
                })
            
            # ä¸€èˆ¬çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«åã®ä¿®æ­£ææ¡ˆ
            common_corrections = {
                "campaign": "LookerStudio_report_campaign",
                "age_group": "LookerStudio_report_age_group",
                "gender": "LookerStudio_report_gender"
            }
            
            for wrong, correct in common_corrections.items():
                if wrong in table.lower():
                    suggestions.append({
                        "type": "table_fix",
                        "description": f"ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ '{correct}' ã«ä¿®æ­£ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™",
                        "suggestion": f"`vorn-digi-mktg-poc-635a.toki_air.{correct}`",
                        "priority": "medium",
                        "auto_fixable": True
                    })
        
        return suggestions
    
    def _suggest_column_fixes(self, sql: str, error_message: str) -> List[Dict]:
        """åˆ—åã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ææ¡ˆ"""
        suggestions = []
        
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æœªèªè­˜ã®åˆ—åã‚’æŠ½å‡º
        column_match = re.search(r'Unrecognized name:\s*([^\s;]+)', error_message, re.IGNORECASE)
        if column_match:
            unrecognized_column = column_match.group(1)
            
            # ä¸€èˆ¬çš„ãªåˆ—åã®ä¿®æ­£ææ¡ˆ
            common_columns = {
                "cost": "CostIncludingFees",
                "clicks": "Clicks", 
                "impressions": "Impressions",
                "conversions": "Conversions",
                "campaign": "CampaignName",
                "date": "Date"
            }
            
            for wrong, correct in common_columns.items():
                if wrong.lower() in unrecognized_column.lower():
                    suggestions.append({
                        "type": "column_fix",
                        "description": f"åˆ—å '{unrecognized_column}' ã‚’ '{correct}' ã«ä¿®æ­£ã—ã¦ãã ã•ã„",
                        "suggestion": correct,
                        "priority": "high",
                        "auto_fixable": True
                    })
        
        return suggestions
    
    def _suggest_groupby_fixes(self, sql: str) -> List[Dict]:
        """GROUP BYé–¢é€£ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ææ¡ˆ"""
        suggestions = []
        
        # SELECTå¥ã®åˆ—ã¨GROUP BYå¥ã®åˆ—ã‚’æ¯”è¼ƒ
        select_match = re.search(r'SELECT\s+(.*?)\s+FROM', sql, re.IGNORECASE | re.DOTALL)
        groupby_match = re.search(r'GROUP\s+BY\s+(.*?)(?:\s+ORDER|\s+LIMIT|\s*$)', sql, re.IGNORECASE | re.DOTALL)
        
        if select_match and groupby_match:
            select_columns = [col.strip() for col in select_match.group(1).split(',')]
            groupby_columns = [col.strip() for col in groupby_match.group(1).split(',')]
            
            # é›†ç´„é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã„ãªã„åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
            non_aggregate_columns = []
            for col in select_columns:
                if not re.search(r'(SUM|COUNT|AVG|MAX|MIN|STDDEV)\s*\(', col, re.IGNORECASE):
                    non_aggregate_columns.append(col)
            
            # GROUP BYã«å«ã¾ã‚Œã¦ã„ãªã„éé›†ç´„åˆ—ã‚’ç‰¹å®š
            missing_columns = []
            for col in non_aggregate_columns:
                col_clean = re.sub(r'\s+AS\s+\w+', '', col, flags=re.IGNORECASE).strip()
                if col_clean not in groupby_columns:
                    missing_columns.append(col_clean)
            
            if missing_columns:
                suggestions.append({
                    "type": "groupby_fix",
                    "description": f"ä»¥ä¸‹ã®åˆ—ã‚’GROUP BYå¥ã«è¿½åŠ ã—ã¦ãã ã•ã„: {', '.join(missing_columns)}",
                    "suggestion": f"GROUP BY {', '.join(groupby_columns + missing_columns)}",
                    "priority": "high",
                    "auto_fixable": True
                })
        
        return suggestions
    
    def _suggest_type_fixes(self, sql: str) -> List[Dict]:
        """ãƒ‡ãƒ¼ã‚¿å‹ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ææ¡ˆ"""
        suggestions = []
        
        # CASTé–¢æ•°ã®ä½¿ç”¨ã‚’ææ¡ˆ
        suggestions.append({
            "type": "type_fix",
            "description": "ãƒ‡ãƒ¼ã‚¿å‹ã®ä¸ä¸€è‡´ãŒã‚ã‚‹å ´åˆã¯ã€CASTé–¢æ•°ã¾ãŸã¯SAFE_CASTé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
            "suggestion": "SAFE_CAST(column_name AS STRING) ã¾ãŸã¯ CAST(column_name AS INT64)",
            "priority": "medium",
            "auto_fixable": False
        })
        
        # NULLå€¤ã®å‡¦ç†ã‚’ææ¡ˆ
        suggestions.append({
            "type": "type_fix",
            "description": "NULLå€¤ãŒåŸå› ã®å ´åˆã¯ã€COALESCEé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
            "suggestion": "COALESCE(column_name, default_value)",
            "priority": "medium",
            "auto_fixable": False
        })
        
        return suggestions

# =========================================================================
# ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºæ©Ÿèƒ½
# =========================================================================

def show_enhanced_error_message(error, context: Dict = None):
    """å¼·åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º"""
    if context is None:
        context = {}
    
    error_handler = EnhancedErrorHandler()
    error_analysis = error_handler.analyze_error(str(error), context)
    
    # ã‚¨ãƒ©ãƒ¼ã®é‡è¦åº¦ã«å¿œã˜ã¦è¡¨ç¤ºè‰²ã‚’å¤‰æ›´
    severity = error_analysis["severity"]
    if severity == "high":
        st.error(f"ğŸ”´ **{error_analysis['category']}**")
    elif severity == "medium":
        st.warning(f"ğŸŸ¡ **{error_analysis['category']}**")
    else:
        st.info(f"â„¹ï¸ **{error_analysis['category']}**")
    
    # å…ƒã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    with st.expander("ğŸ“‹ è©³ç´°ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", expanded=False):
        st.code(error_analysis["original_message"])
    
    # è§£æ±ºç­–ã®è¡¨ç¤º
    st.markdown("### ğŸ’¡ æ¨å¥¨è§£æ±ºç­–")
    for i, solution in enumerate(error_analysis["solutions"], 1):
        st.markdown(f"{i}. {solution}")
    
    # è‡ªå‹•ä¿®æ­£ææ¡ˆ
    if error_analysis["auto_fix_suggestions"]:
        st.markdown("### ğŸ”§ è‡ªå‹•ä¿®æ­£ææ¡ˆ")
        
        for suggestion in error_analysis["auto_fix_suggestions"]:
            priority_emoji = "ğŸ”´" if suggestion["priority"] == "high" else "ğŸŸ¡" if suggestion["priority"] == "medium" else "ğŸŸ¢"
            
            with st.expander(f"{priority_emoji} {suggestion['description']}", expanded=suggestion["priority"] == "high"):
                if suggestion.get("suggestion"):
                    st.code(suggestion["suggestion"], language="sql")
                
                if suggestion.get("auto_fixable", False):
                    if st.button(f"è‡ªå‹•ä¿®æ­£ã‚’é©ç”¨", key=f"fix_{hash(suggestion['description'])}"):
                        st.info("è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ã¯é–‹ç™ºä¸­ã§ã™")
    
    # ã‚¨ãƒ©ãƒ¼å±¥æ­´ã¸ã®è¿½åŠ 
    try:
        if "error_history" not in st.session_state:
            st.session_state.error_history = []
        
        st.session_state.error_history.append({
            "timestamp": datetime.now(),
            "category": error_analysis["category"],
            "original_message": error_analysis["original_message"],
            "severity": error_analysis["severity"],
            "solutions": error_analysis["solutions"]
        })
        
        # å±¥æ­´ã®æœ€å¤§æ•°ã‚’åˆ¶é™
        if len(st.session_state.error_history) > 50:
            st.session_state.error_history = st.session_state.error_history[-50:]
            
    except Exception as e:
        pass  # ã‚¨ãƒ©ãƒ¼å±¥æ­´ã®è¿½åŠ ã«å¤±æ•—ã—ã¦ã‚‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åœæ­¢ã•ã›ãªã„

# =========================================================================
# ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½
# =========================================================================

def suggest_error_recovery(error_analysis: Dict, original_sql: str = "", user_input: str = "") -> List[Dict]:
    """ã‚¨ãƒ©ãƒ¼å›å¾©ã®ãŸã‚ã®å…·ä½“çš„ãªææ¡ˆ"""
    recovery_options = []
    
    category = error_analysis.get("category", "")
    
    if "æ§‹æ–‡ã‚¨ãƒ©ãƒ¼" in category:
        recovery_options.extend([
            {
                "title": "SQLæ§‹æ–‡ãƒã‚§ãƒƒã‚«ãƒ¼ä½¿ç”¨",
                "description": "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®SQLæ§‹æ–‡ãƒã‚§ãƒƒã‚«ãƒ¼ã§SQLã‚’æ¤œè¨¼",
                "action_type": "external_tool",
                "url": "https://www.eversql.com/sql-syntax-check-validator/"
            },
            {
                "title": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†å®Ÿè¡Œ", 
                "description": "ã‚ˆã‚Šå…·ä½“çš„ãªæŒ‡ç¤ºã§AIã«SQLå†ç”Ÿæˆã‚’ä¾é ¼",
                "action_type": "regenerate",
                "suggestion": f"ä»¥ä¸‹ã®æŒ‡ç¤ºã‚’ã‚ˆã‚Šè©³ç´°ã«æ›¸ãæ›ãˆã¦ãã ã•ã„: {user_input}"
            }
        ])
    
    elif "ãƒ†ãƒ¼ãƒ–ãƒ«" in category:
        recovery_options.extend([
            {
                "title": "ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª",
                "description": "BigQueryã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª",
                "action_type": "manual_check",
                "sql_suggestion": "SELECT table_name FROM `vorn-digi-mktg-poc-635a.toki_air.INFORMATION_SCHEMA.TABLES`"
            },
            {
                "title": "æ¨©é™ç¢ºèª",
                "description": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ç®¡ç†è€…ã«ç¢ºèªä¾é ¼",
                "action_type": "permission_check"
            }
        ])
    
    elif "åˆ—å" in category:
        recovery_options.extend([
            {
                "title": "ã‚¹ã‚­ãƒ¼ãƒç¢ºèª",
                "description": "ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ—æ§‹é€ ã‚’ç¢ºèª",
                "action_type": "schema_check",
                "sql_suggestion": "SELECT column_name, data_type FROM `vorn-digi-mktg-poc-635a.toki_air.INFORMATION_SCHEMA.COLUMNS` WHERE table_name = 'LookerStudio_report_campaign'"
            }
        ])
    
    return recovery_options

def create_error_report(error_history: List[Dict]) -> str:
    """ã‚¨ãƒ©ãƒ¼å±¥æ­´ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    if not error_history:
        return "ã‚¨ãƒ©ãƒ¼å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    
    # ã‚¨ãƒ©ãƒ¼çµ±è¨ˆã®è¨ˆç®—
    total_errors = len(error_history)
    error_categories = {}
    severity_counts = {"high": 0, "medium": 0, "low": 0}
    
    for error in error_history:
        category = error.get("category", "æœªåˆ†é¡")
        severity = error.get("severity", "medium")
        
        error_categories[category] = error_categories.get(category, 0) + 1
        severity_counts[severity] = severity_counts.get(severity, 0) + 1
    
    # ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    report = f"""
# ã‚¨ãƒ©ãƒ¼åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š çµ±è¨ˆæƒ…å ±
- **ç·ã‚¨ãƒ©ãƒ¼æ•°**: {total_errors}
- **é«˜é‡è¦åº¦**: {severity_counts['high']}
- **ä¸­é‡è¦åº¦**: {severity_counts['medium']}  
- **ä½é‡è¦åº¦**: {severity_counts['low']}

## ğŸ“‹ ã‚¨ãƒ©ãƒ¼ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
"""
    
    for category, count in sorted(error_categories.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_errors) * 100
        report += f"- **{category}**: {count}å› ({percentage:.1f}%)\n"
    
    report += f"""
## ğŸ”„ æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ï¼ˆæœ€æ–°5ä»¶ï¼‰
"""
    
    recent_errors = error_history[-5:]
    for i, error in enumerate(reversed(recent_errors), 1):
        timestamp = error["timestamp"].strftime("%Y-%m-%d %H:%M:%S")
        report += f"{i}. **{error['category']}** ({timestamp})\n   {error['original_message'][:100]}...\n\n"
    
    return report

# =========================================================================
# ãƒ‡ãƒãƒƒã‚°æ”¯æ´æ©Ÿèƒ½
# =========================================================================

def debug_sql_execution(sql: str, bq_client) -> Dict:
    """SQLå®Ÿè¡Œã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’åé›†"""
    debug_info = {
        "sql_length": len(sql),
        "sql_complexity": _analyze_sql_complexity(sql),
        "estimated_cost": "ä¸æ˜",
        "syntax_valid": False,
        "dry_run_result": None
    }
    
    try:
        # åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        from analysis_controller import validate_basic_sql_syntax
        debug_info["syntax_valid"] = validate_basic_sql_syntax(sql)
    except:
        debug_info["syntax_valid"] = False
    
    try:
        # BigQueryã®ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ
        if bq_client:
            job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
            query_job = bq_client.query(sql, job_config=job_config)
            
            debug_info["estimated_cost"] = f"{query_job.total_bytes_processed / 1024 / 1024:.2f} MB"
            debug_info["dry_run_result"] = "æˆåŠŸ"
    except Exception as e:
        debug_info["dry_run_result"] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    return debug_info

def _analyze_sql_complexity(sql: str) -> str:
    """SQLè¤‡é›‘åº¦ã®åˆ†æ"""
    complexity_factors = {
        "joins": len(re.findall(r'\bJOIN\b', sql, re.IGNORECASE)),
        "subqueries": len(re.findall(r'\bSELECT\b', sql, re.IGNORECASE)) - 1,
        "aggregations": len(re.findall(r'\b(SUM|COUNT|AVG|MAX|MIN)\s*\(', sql, re.IGNORECASE)),
        "window_functions": len(re.findall(r'\bOVER\s*\(', sql, re.IGNORECASE))
    }
    
    total_complexity = sum(complexity_factors.values())
    
    if total_complexity == 0:
        return "ã‚·ãƒ³ãƒ—ãƒ«"
    elif total_complexity <= 3:
        return "æ™®é€š"
    elif total_complexity <= 7:
        return "è¤‡é›‘"
    else:
        return "éå¸¸ã«è¤‡é›‘"

# =========================================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
# =========================================================================

def export_error_data(error_history: List[Dict], format_type: str = "json") -> str:
    """ã‚¨ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    if format_type == "json":
        return json.dumps(error_history, ensure_ascii=False, indent=2, default=str)
    elif format_type == "csv":
        import pandas as pd
        df = pd.DataFrame(error_history)
        return df.to_csv(index=False)
    else:
        return create_error_report(error_history)

# =========================================================================
# çµ±åˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°é–¢æ•°
# =========================================================================

def handle_application_error(error: Exception, context: Dict = None, show_ui: bool = True) -> Dict:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã§ä½¿ç”¨ã™ã‚‹çµ±åˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
    try:
        error_handler = EnhancedErrorHandler()
        error_analysis = error_handler.analyze_error(str(error), context or {})
        
        if show_ui:
            show_enhanced_error_message(error, context)
        
        # ãƒ­ã‚°è¨˜éŒ²
        error_log = {
            "timestamp": datetime.now().isoformat(),
            "error_type": type(error).__name__,
            "error_analysis": error_analysis,
            "context": context or {},
            "traceback": traceback.format_exc()
        }
        
        return error_log
        
    except Exception as handling_error:
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è‡ªä½“ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        fallback_message = f"ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(handling_error)}"
        if show_ui:
            st.error(f"âŒ {fallback_message}")
            st.error(f"å…ƒã®ã‚¨ãƒ©ãƒ¼: {str(error)}")
        
        return {
            "timestamp": datetime.now().isoformat(),
            "error_type": "ErrorHandlingFailure",
            "original_error": str(error),
            "handling_error": fallback_message
        }