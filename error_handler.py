# error_handler.py - ÊßãÊñá„Ç®„É©„Éº‰øÆÊ≠£Áâà
"""
Âº∑Âåñ„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞Ê©üËÉΩ
- Ë©≥Á¥∞„Å™„Ç®„É©„ÉºÂàÜÊûê„Å®ÂàÜÈ°û
- ÂÖ∑‰ΩìÁöÑ„Å™Ëß£Ê±∫Á≠ñ„ÅÆÊèêÁ§∫
- „Ç®„É©„Éº„ÅÆËá™Âãï‰øÆÊ≠£Ê©üËÉΩ
- „Ç®„É©„ÉºÂ±•Ê≠¥„ÅÆËøΩË∑°
"""

import streamlit as st
import pandas as pd
import re
import traceback
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import json

class EnhancedErrorHandler:
    """Âº∑Âåñ„Åï„Çå„Åü„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÇØ„É©„Çπ"""
    
    def __init__(self):
        # „Ç®„É©„Éº„Éë„Çø„Éº„É≥„Å®Ëß£Ê±∫Á≠ñ„ÅÆ„Éû„ÉÉ„Éî„É≥„Ç∞
        self.error_patterns = {
            # BigQuery SQL„Ç®„É©„Éº
            "syntax_error": {
                "patterns": ["Syntax error", "syntax", "Expected", "Unexpected"],
                "category": "SQLÊßãÊñá„Ç®„É©„Éº",
                "severity": "high",
                "solutions": [
                    "SQLÊßãÊñá„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "„Ç´„É≥„Éû„ÄÅÊã¨Âºß„ÄÅ„ÇØ„Ç©„Éº„Éà„ÅÆÂØæÂøú„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "‰∫àÁ¥ÑË™û„ÅÆ‰ΩøÁî®ÊñπÊ≥ï„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ]
            },
            
            "table_not_found": {
                "patterns": ["not found", "does not exist", "Table", "Dataset"],
                "category": "„ÉÜ„Éº„Éñ„É´„Éª„Éá„Éº„Çø„Çª„ÉÉ„Éà„Ç®„É©„Éº",
                "severity": "high",
                "solutions": [
                    "„ÉÜ„Éº„Éñ„É´Âêç„ÅÆ„Çπ„Éö„É´„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂêç„ÅåÊ≠£„Åó„ÅÑ„ÅãÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "„ÉÜ„Éº„Éñ„É´„Å∏„ÅÆ„Ç¢„ÇØ„Çª„ÇπÊ®©Èôê„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ]
            },
            
            "column_not_found": {
                "patterns": ["Unrecognized name", "Column", "field"],
                "category": "ÂàóÂêç„Ç®„É©„Éº",
                "severity": "medium",
                "solutions": [
                    "ÂàóÂêç„ÅÆ„Çπ„Éö„É´„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "ÂàóÂêç„ÅØÂ§ßÊñáÂ≠óÂ∞èÊñáÂ≠ó„ÇíÂå∫Âà•„Åó„Åæ„Åô",
                    "Âà©Áî®ÂèØËÉΩ„Å™ÂàóÂêç„ÇíDESCRIBE„ÇØ„Ç®„É™„ÅßÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ]
            },
            
            "type_mismatch": {
                "patterns": ["type", "cannot be coerced", "CAST", "conversion"],
                "category": "„Éá„Éº„ÇøÂûã„Ç®„É©„Éº",
                "severity": "medium",
                "solutions": [
                    "CASTÈñ¢Êï∞„Çí‰ΩøÁî®„Åó„Å¶ÈÅ©Âàá„Å™Âûã„Å´Â§âÊèõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "NULLÂÄ§„ÅÆÂá¶ÁêÜ„ÇíËøΩÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "SAFE_CASTÈñ¢Êï∞„ÅÆ‰ΩøÁî®„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ]
            },
            
            "aggregate_error": {
                "patterns": ["GROUP BY", "aggregate", "must be grouped"],
                "category": "ÈõÜÁ¥ÑÈñ¢Êï∞„Ç®„É©„Éº",
                "severity": "medium",
                "solutions": [
                    "GROUP BYÂè•„Å´ÂøÖË¶Å„Å™Âàó„ÇíËøΩÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "ÈõÜÁ¥ÑÈñ¢Êï∞„ÇíÈÅ©Âàá„Å´‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "ÈùûÈõÜÁ¥ÑÂàó„ÇíGROUP BYÂè•„Å´Âê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ"
                ]
            },
            
            "quota_exceeded": {
                "patterns": ["quota", "exceeded", "limit", "timeout"],
                "category": "„É™„ÇΩ„Éº„ÇπÂà∂Èôê„Ç®„É©„Éº",
                "severity": "high",
                "solutions": [
                    "„ÇØ„Ç®„É™„ÅÆÁØÑÂõ≤„ÇíÁã≠„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑÔºàÊó•‰ªòÊù°‰ª∂„ÅÆËøΩÂä†Ôºâ",
                    "LIMITÂè•„Çí‰ΩøÁî®„Åó„Å¶ÁµêÊûúÊï∞„ÇíÂà∂Èôê„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "ÂøÖË¶Å„Å™Âàó„ÅÆ„Åø„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ]
            },
            
            "permission_error": {
                "patterns": ["permission", "access", "denied", "forbidden"],
                "category": "Ê®©Èôê„Ç®„É©„Éº",
                "severity": "high",
                "solutions": [
                    "„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å∏„ÅÆ„Ç¢„ÇØ„Çª„ÇπÊ®©Èôê„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "BigQuery„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆË®≠ÂÆö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "ÁÆ°ÁêÜËÄÖ„Å´Ê®©Èôê„ÅÆ‰ªò‰∏é„Çí‰æùÈ†º„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ]
            },
            
            "api_error": {
                "patterns": ["API", "connection", "network", "timeout"],
                "category": "API„ÉªÊé•Á∂ö„Ç®„É©„Éº",
                "severity": "high",
                "solutions": [
                    "„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊé•Á∂ö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "APIË™çË®ºÊÉÖÂ†±„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "„Åó„Å∞„Çâ„ÅèÊôÇÈñì„Çí„Åä„ÅÑ„Å¶ÂÜçË©¶Ë°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ]
            }
        }
        
        # „Ç®„É©„ÉºÂ±•Ê≠¥
        if "error_history" not in st.session_state:
            st.session_state.error_history = []
    
    def analyze_error(self, error_message: str, error_context: Dict = None) -> Dict[str, Any]:
        """„Ç®„É©„Éº„ÅÆË©≥Á¥∞ÂàÜÊûê"""
        error_context = error_context or {}
        
        # „Ç®„É©„Éº„Éë„Çø„Éº„É≥„ÅÆ„Éû„ÉÉ„ÉÅ„É≥„Ç∞
        matched_pattern = self._match_error_pattern(error_message)
        
        # „Ç®„É©„Éº„ÅÆÂàÜÈ°û„Å®Ë©≥Á¥∞ÂàÜÊûê
        analysis = {
            "original_message": error_message,
            "pattern_match": matched_pattern,
            "category": matched_pattern["category"] if matched_pattern else "‰∏çÊòé„Å™„Ç®„É©„Éº",
            "severity": matched_pattern["severity"] if matched_pattern else "medium",
            "timestamp": datetime.now(),
            "context": error_context,
            "solutions": matched_pattern["solutions"] if matched_pattern else ["„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÁ¢∫Ë™ç„Åó„ÄÅ‰∏ÄËà¨ÁöÑ„Å™SQLÊßãÊñá„É´„Éº„É´„Å´Âæì„Å£„Å¶„Åè„Å†„Åï„ÅÑ"],
            "auto_fix_suggestions": self._generate_auto_fix_suggestions(error_message, error_context)
        }
        
        # „Ç®„É©„ÉºÂ±•Ê≠¥„Å´ËøΩÂä†
        self._add_to_history(analysis)
        
        return analysis
    
    def _match_error_pattern(self, error_message: str) -> Optional[Dict]:
        """„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„Å®„Éë„Çø„Éº„É≥„ÅÆ„Éû„ÉÉ„ÉÅ„É≥„Ç∞"""
        error_lower = error_message.lower()
        
        for pattern_name, pattern_info in self.error_patterns.items():
            for pattern in pattern_info["patterns"]:
                if pattern.lower() in error_lower:
                    return pattern_info
        
        return None
    
    def _generate_auto_fix_suggestions(self, error_message: str, context: Dict) -> List[Dict]:
        """Ëá™Âãï‰øÆÊ≠£ÊèêÊ°à„ÅÆÁîüÊàê"""
        suggestions = []
        
        # SQLÊñá„ÅÆÂèñÂæó
        sql = context.get("sql", "")
        user_input = context.get("user_input", "")
        
        # ‰∏ÄËà¨ÁöÑ„Å™‰øÆÊ≠£„Éë„Çø„Éº„É≥
        if "syntax error" in error_message.lower():
            suggestions.extend(self._suggest_syntax_fixes(sql))
        
        if "not found" in error_message.lower() and "table" in error_message.lower():
            suggestions.extend(self._suggest_table_fixes(sql))
        
        if "unrecognized name" in error_message.lower():
            suggestions.extend(self._suggest_column_fixes(sql, error_message))
        
        if "group by" in error_message.lower():
            suggestions.extend(self._suggest_groupby_fixes(sql))
        
        if "cast" in error_message.lower() or "type" in error_message.lower():
            suggestions.extend(self._suggest_type_fixes(sql))
        
        return suggestions
    
    def _suggest_syntax_fixes(self, sql: str) -> List[Dict]:
        """ÊßãÊñá„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£ÊèêÊ°à"""
        suggestions = []
        
        # ‰∏ÄËà¨ÁöÑ„Å™ÊßãÊñáÂïèÈ°å„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ
        issues = []
        
        # „Ç´„É≥„Éû„ÅÆÂïèÈ°å
        if sql.count("SELECT") > 0:
            # SELECTÂè•„Åß„ÅÆÊú´Â∞æ„Ç´„É≥„Éû„ÉÅ„Çß„ÉÉ„ÇØ
            select_match = re.search(r'SELECT\s+.*?FROM', sql, re.IGNORECASE | re.DOTALL)
            if select_match:
                select_part = select_match.group()
                if re.search(r',\s*FROM', select_part, re.IGNORECASE):
                    issues.append("SELECTÂè•„ÅÆÊú´Â∞æ„Å´‰∏çË¶Å„Å™„Ç´„É≥„Éû„Åå„ÅÇ„Çä„Åæ„Åô")
        
        # Êã¨Âºß„ÅÆÂØæÂøú„ÉÅ„Çß„ÉÉ„ÇØ
        open_parens = sql.count('(')
        close_parens = sql.count(')')
        if open_parens != close_parens:
            issues.append(f"Êã¨Âºß„ÅÆÂØæÂøú„ÅåÂèñ„Çå„Å¶„ÅÑ„Åæ„Åõ„ÇìÔºàÈñãÊã¨Âºß: {open_parens}, ÈñâÊã¨Âºß: {close_parens}Ôºâ")
        
        # „ÇØ„Ç©„Éº„Éà„ÅÆÂØæÂøú„ÉÅ„Çß„ÉÉ„ÇØ
        single_quotes = sql.count("'")
        if single_quotes % 2 != 0:
            issues.append("„Ç∑„É≥„Ç∞„É´„ÇØ„Ç©„Éº„Éà„ÅÆÂØæÂøú„ÅåÂèñ„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì")
        
        # ‰øÆÊ≠£ÊèêÊ°à„ÅÆÁîüÊàê
        for issue in issues:
            suggestions.append({
                "type": "syntax_fix",
                "description": issue,
                "priority": "high",
                "auto_fixable": False
            })
        
        return suggestions
    
    def _suggest_table_fixes(self, sql: str) -> List[Dict]:
        """„ÉÜ„Éº„Éñ„É´Èñ¢ÈÄ£„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£ÊèêÊ°à"""
        suggestions = []
        
        # „ÉÜ„Éº„Éñ„É´Âêç„ÅÆ„Éë„Çø„Éº„É≥ÊäΩÂá∫
        table_patterns = re.findall(r'FROM\s+`?([^`\s]+)`?', sql, re.IGNORECASE)
        
        for table in table_patterns:
            # Ê≠£„Åó„ÅÑ„ÉÜ„Éº„Éñ„É´Âêç„ÅÆÊé®Ê∏¨
            if "." not in table:
                suggestions.append({
                    "type": "table_fix",
                    "description": f"„ÉÜ„Éº„Éñ„É´Âêç '{table}' „Å´„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Éª„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂêç„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô",
                    "suggested_fix": f"FROM `project.dataset.{table}`",
                    "priority": "high",
                    "auto_fixable": True
                })
            
            # ‰∏ÄËà¨ÁöÑ„Å™„ÉÜ„Éº„Éñ„É´Âêç„ÅÆÈñìÈÅï„ÅÑ
            common_fixes = {
                "campaign": "LookerStudio_report_campaign",
                "report": "LookerStudio_report_campaign"
            }
            
            for wrong, correct in common_fixes.items():
                if wrong in table.lower():
                    suggestions.append({
                        "type": "table_fix",
                        "description": f"„ÉÜ„Éº„Éñ„É´Âêç '{table}' „ÅØ '{correct}' „ÅÆÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô",
                        "suggested_fix": table.replace(wrong, correct),
                        "priority": "medium",
                        "auto_fixable": True
                    })
        
        return suggestions
    
    def _suggest_column_fixes(self, sql: str, error_message: str) -> List[Dict]:
        """ÂàóÂêç„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£ÊèêÊ°à"""
        suggestions = []
        
        # „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„Åã„ÇâÂàóÂêç„ÇíÊäΩÂá∫
        column_match = re.search(r"name:\s*([^\s;]+)", error_message)
        if column_match:
            problematic_column = column_match.group(1)
            
            # ‰∏ÄËà¨ÁöÑ„Å™ÂàóÂêç„ÅÆ‰øÆÊ≠£ÊèêÊ°à
            common_column_fixes = {
                "cost": "CostIncludingFees",
                "click": "Clicks",
                "impression": "Impressions",
                "conversion": "Conversions",
                "ctr": "Clicks / Impressions * 100",
                "cpa": "CostIncludingFees / Conversions",
                "cpc": "CostIncludingFees / Clicks",
                "cvr": "Conversions / Clicks * 100",
                "roas": "ConversionValue / CostIncludingFees"
            }
            
            for wrong, correct in common_column_fixes.items():
                if wrong.lower() in problematic_column.lower():
                    suggestions.append({
                        "type": "column_fix",
                        "description": f"ÂàóÂêç '{problematic_column}' „ÅØ '{correct}' „ÅÆÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô",
                        "suggested_fix": correct,
                        "priority": "high",
                        "auto_fixable": True
                    })
        
        return suggestions
    
    def _suggest_groupby_fixes(self, sql: str) -> List[Dict]:
        """GROUP BYÈñ¢ÈÄ£„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£ÊèêÊ°à"""
        suggestions = []
        
        # SELECTÂè•„ÅÆÂàó„ÇíÊäΩÂá∫
        select_match = re.search(r'SELECT\s+(.*?)\s+FROM', sql, re.IGNORECASE | re.DOTALL)
        if select_match:
            select_clause = select_match.group(1)
            
            # ÈõÜÁ¥ÑÈñ¢Êï∞‰ª•Â§ñ„ÅÆÂàó„ÇíÁâπÂÆö
            non_aggregate_columns = []
            aggregate_functions = ['SUM', 'COUNT', 'AVG', 'MAX', 'MIN', 'STDDEV']
            
            # Á∞°ÊòìÁöÑ„Å™ÂàóÊäΩÂá∫ÔºàÂÆüÈöõ„ÅØ„Çà„ÇäË§áÈõë„Å™Ëß£Êûê„ÅåÂøÖË¶ÅÔºâ
            columns = [col.strip() for col in select_clause.split(',')]
            
            for col in columns:
                col_upper = col.upper()
                is_aggregate = any(func in col_upper for func in aggregate_functions)
                
                if not is_aggregate and 'AS ' not in col_upper:
                    # Ë®àÁÆóÂàó„Åß„Å™„ÅÑÂ†¥Âêà„ÅØGROUP BY„ÅåÂøÖË¶Å
                    # ‰øÆÊ≠£: column_name = col.split('.')[- ] ‚Üí column_name = col.split('.')[-1]
                    column_name = col.split('.')[-1]  # ÊúÄÂæå„ÅÆË¶ÅÁ¥†„ÇíÂèñÂæó
                    if column_name not in ['*']:
                        non_aggregate_columns.append(column_name)
            
            if non_aggregate_columns:
                suggestions.append({
                    "type": "groupby_fix",
                    "description": "GROUP BYÂè•„Å´ÈùûÈõÜÁ¥ÑÂàó„ÇíËøΩÂä†„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô",
                    "suggested_fix": f"GROUP BY {', '.join(non_aggregate_columns)}",
                    "priority": "high",
                    "auto_fixable": True
                })
        
        return suggestions
    
    def _suggest_type_fixes(self, sql: str) -> List[Dict]:
        """„Éá„Éº„ÇøÂûãÈñ¢ÈÄ£„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£ÊèêÊ°à"""
        suggestions = []
        
        # SAFE_DIVIDEÁ≠â„ÅÆÂÆâÂÖ®„Å™Èñ¢Êï∞„ÅÆ‰ΩøÁî®„ÇíÊèêÊ°à
        if "/" in sql:
            suggestions.append({
                "type": "type_fix",
                "description": "Èô§ÁÆó„Åß„Çº„É≠Èô§ÁÆó„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô",
                "suggested_fix": "SAFE_DIVIDE(ÂàÜÂ≠ê, ÂàÜÊØç) „Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "priority": "medium",
                "auto_fixable": False
            })
        
        # NULLÂÄ§Âá¶ÁêÜ„ÅÆÊèêÊ°à
        suggestions.append({
            "type": "type_fix",
            "description": "NULLÂÄ§„ÅåÂéüÂõ†„Åß„Éá„Éº„ÇøÂûã„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô",
            "suggested_fix": "COALESCE(ÂàóÂêç, „Éá„Éï„Ç©„É´„ÉàÂÄ§) „ÇÑ IS NOT NULL „Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
            "priority": "medium",
            "auto_fixable": False
        })
        
        return suggestions
    
    def _add_to_history(self, analysis: Dict):
        """„Ç®„É©„ÉºÂ±•Ê≠¥„Å∏„ÅÆËøΩÂä†"""
        st.session_state.error_history.append(analysis)
        
        # Â±•Ê≠¥„ÅÆ‰∏äÈôêÂà∂Âæ°
        if len(st.session_state.error_history) > 50:
            st.session_state.error_history = st.session_state.error_history[-50:]
    
    def apply_auto_fix(self, original_sql: str, fix_suggestion: Dict) -> str:
        """Ëá™Âãï‰øÆÊ≠£„ÅÆÈÅ©Áî®"""
        if not fix_suggestion.get("auto_fixable", False):
            return original_sql
        
        fix_type = fix_suggestion["type"]
        suggested_fix = fix_suggestion.get("suggested_fix", "")
        
        if fix_type == "table_fix":
            # „ÉÜ„Éº„Éñ„É´Âêç„ÅÆ‰øÆÊ≠£
            if "." not in original_sql and "`" not in original_sql:
                # „Éó„É≠„Ç∏„Çß„ÇØ„Éà„Éª„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂêç„ÅÆËøΩÂä†
                corrected_sql = re.sub(
                    r'FROM\s+(\w+)',
                    f'FROM `vorn-digi-mktg-poc-635a.toki_air.\\1`',
                    original_sql,
                    flags=re.IGNORECASE
                )
                return corrected_sql
        
        elif fix_type == "groupby_fix":
            # GROUP BYÂè•„ÅÆËøΩÂä†
            if "GROUP BY" not in original_sql.upper():
                return original_sql + "\n" + suggested_fix
        
        return original_sql
    
    def get_error_statistics(self) -> Dict[str, Any]:
        """„Ç®„É©„ÉºÁµ±Ë®à„ÅÆÂèñÂæó"""
        if not st.session_state.error_history:
            return {"total_errors": 0}
        
        history = st.session_state.error_history
        
        # „Ç´„ÉÜ„Ç¥„É™Âà•„Ç®„É©„ÉºÊï∞
        category_counts = {}
        severity_counts = {}
        
        for error in history:
            category = error["category"]
            severity = error["severity"]
            
            category_counts[category] = category_counts.get(category, 0) + 1
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
        
        # ÊúÄÈ†ªÂá∫„Ç®„É©„Éº
        most_common_category = max(category_counts.items(), key=lambda x: x[1]) if category_counts else None
        
        return {
            "total_errors": len(history),
            "category_distribution": category_counts,
            "severity_distribution": severity_counts,
            "most_common_error": most_common_category,
            "recent_errors": history[-5:]  # ÊúÄÊñ∞5‰ª∂
        }

# =========================================================================
# „Ç®„É©„ÉºË°®Á§∫„ÉªÂá¶ÁêÜÁî®„ÅÆStreamlitÈñ¢Êï∞
# =========================================================================

def show_enhanced_error_message(error: Exception, context: Dict = None):
    """Âº∑Âåñ„Åï„Çå„Åü„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆË°®Á§∫"""
    handler = EnhancedErrorHandler()
    
    # „Ç®„É©„Éº„ÅÆÂàÜÊûê
    error_analysis = handler.analyze_error(str(error), context)
    
    # „Ç®„É©„Éº„ÅÆÈáçË¶ÅÂ∫¶„Å´Âøú„Åò„ÅüË°®Á§∫
    severity = error_analysis["severity"]
    category = error_analysis["category"]
    
    if severity == "high":
        st.error(f"üî¥ **{category}**")
    elif severity == "medium":
        st.warning(f"üü° **{category}**")
    else:
        st.info(f"‚ÑπÔ∏è **{category}**")
    
    # „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆË°®Á§∫
    with st.expander("üìã „Ç®„É©„ÉºË©≥Á¥∞", expanded=True):
        st.text(error_analysis["original_message"])
    
    # Ëß£Ê±∫Á≠ñ„ÅÆË°®Á§∫
    st.markdown("### üí° Ëß£Ê±∫Á≠ñ")
    for i, solution in enumerate(error_analysis["solutions"], 1):
        st.markdown(f"{i}. {solution}")
    
    # Ëá™Âãï‰øÆÊ≠£ÊèêÊ°à„ÅÆË°®Á§∫
    if error_analysis["auto_fix_suggestions"]:
        st.markdown("### üîß Ëá™Âãï‰øÆÊ≠£ÊèêÊ°à")
        
        for suggestion in error_analysis["auto_fix_suggestions"]:
            priority_icon = "üî¥" if suggestion["priority"] == "high" else "üü°" if suggestion["priority"] == "medium" else "üü¢"
            
            st.markdown(f"**{priority_icon} {suggestion['description']}**")
            
            if suggestion.get("suggested_fix"):
                st.code(suggestion["suggested_fix"], language="sql")
            
            # Ëá™Âãï‰øÆÊ≠£„Éú„Çø„É≥
            if suggestion.get("auto_fixable", False) and context and context.get("sql"):
                if st.button(f"üöÄ „Åì„ÅÆ‰øÆÊ≠£„ÇíÈÅ©Áî®", key=f"fix_{hash(suggestion['description'])}"):
                    fixed_sql = handler.apply_auto_fix(context["sql"], suggestion)
                    st.session_state.sql = fixed_sql
                    st.session_state.editable_sql = fixed_sql
                    st.success("‚úÖ ‰øÆÊ≠£„ÅåÈÅ©Áî®„Åï„Çå„Åæ„Åó„ÅüÔºÅSQL„ÇíÂÜçÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
                    st.rerun()

def show_error_dashboard():
    """„Ç®„É©„Éº„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„ÅÆË°®Á§∫"""
    handler = EnhancedErrorHandler()
    stats = handler.get_error_statistics()
    
    if stats["total_errors"] == 0:
        st.success("üéâ „Ç®„É©„ÉºÂ±•Ê≠¥„ÅØ„ÅÇ„Çä„Åæ„Åõ„ÇìÔºÅ")
        return
    
    st.markdown("### üìä „Ç®„É©„ÉºÁµ±Ë®à„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ")
    
    # Âü∫Êú¨Áµ±Ë®à
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Á∑è„Ç®„É©„ÉºÊï∞", stats["total_errors"])
    
    with col2:
        if stats.get("most_common_error"):
            st.metric("ÊúÄÈ†ªÂá∫„Ç®„É©„Éº", stats["most_common_error"][0])
    
    with col3:
        high_severity = stats.get("severity_distribution", {}).get("high", 0)
        st.metric("ÈáçË¶Å„Ç®„É©„ÉºÊï∞", high_severity)
    
    # „Ç®„É©„Éº„Ç´„ÉÜ„Ç¥„É™ÂàÜÂ∏É
    if stats.get("category_distribution"):
        st.markdown("#### üìã „Ç®„É©„Éº„Ç´„ÉÜ„Ç¥„É™ÂàÜÂ∏É")
        
        try:
            import plotly.express as px
            
            categories = list(stats["category_distribution"].keys())
            counts = list(stats["category_distribution"].values())
            
            fig = px.pie(
                values=counts,
                names=categories,
                title="„Ç®„É©„Éº„Ç´„ÉÜ„Ç¥„É™Âà•ÂàÜÂ∏É"
            )
            st.plotly_chart(fig, use_container_width=True)
        except ImportError:
            # plotly„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅÆ‰ª£ÊõøË°®Á§∫
            for category, count in stats["category_distribution"].items():
                st.text(f"{category}: {count}‰ª∂")
    
    # ÊúÄËøë„ÅÆ„Ç®„É©„ÉºÂ±•Ê≠¥
    if stats.get("recent_errors"):
        st.markdown("#### üïí ÊúÄËøë„ÅÆ„Ç®„É©„ÉºÂ±•Ê≠¥")
        
        for error in reversed(stats["recent_errors"]):
            timestamp = error["timestamp"].strftime("%Y-%m-%d %H:%M:%S")
            severity_icon = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(error["severity"], "‚ùì")
            
            with st.expander(f"{severity_icon} {timestamp} - {error['category']}"):
                st.text(error["original_message"])
                
                if error.get("solutions"):
                    st.markdown("**ÊèêÊ°à„Åï„Çå„ÅüËß£Ê±∫Á≠ñ:**")
                    for solution in error["solutions"]:
                        st.markdown(f"‚Ä¢ {solution}")

# „Ç∞„É≠„Éº„Éê„É´„Ç§„É≥„Çπ„Çø„É≥„Çπ
error_handler = EnhancedErrorHandler()

# =========================================================================
# Êó¢Â≠ò„Ç≥„Éº„Éâ„Å®„ÅÆ‰∫íÊèõÊÄß„Çí‰øù„Å§„Åü„ÇÅ„ÅÆÈñ¢Êï∞
# =========================================================================

def handle_analysis_error(error: Exception, sql: str = "", operation: str = "") -> bool:
    """ÂàÜÊûêÂá¶ÁêÜ„Åß„ÅÆ„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞ÔºàÊó¢Â≠ò„Ç≥„Éº„Éâ„Å®„ÅÆÁµ±ÂêàÁî®Ôºâ"""
    context = {
        "sql": sql,
        "operation": operation,
        "timestamp": datetime.now()
    }
    
    # Âº∑Âåñ„Åï„Çå„Åü„Ç®„É©„ÉºË°®Á§∫„Çí‰ΩøÁî®
    show_enhanced_error_message(error, context)
    
    # „Ç®„É©„Éº„ÅåÈáçÂ§ß„Åã„Å©„ÅÜ„Åã„ÇíÂà§ÂÆöÔºàÂá¶ÁêÜÁ∂öË°å„ÅÆÂèØÂê¶Ôºâ
    handler = EnhancedErrorHandler()
    error_analysis = handler.analyze_error(str(error), context)
    
    # critical „Åæ„Åü„ÅØ high „ÅÆÂ†¥Âêà„ÅØÂá¶ÁêÜ„ÇíÂÅúÊ≠¢„Åô„Åπ„Åç
    return error_analysis.get("severity") not in ["critical", "high"]

def log_error_to_history(error_message: str, error_category: str = "‰∏ÄËà¨„Ç®„É©„Éº", solutions: List[str] = None):
    """„Ç®„É©„Éº„ÇíÂ±•Ê≠¥„Å´ËøΩÂä†ÔºàÊó¢Â≠ò„Ç≥„Éº„Éâ„Å®„ÅÆ‰∫íÊèõÊÄßÁî®Ôºâ"""
    if "error_history" not in st.session_state:
        st.session_state.error_history = []
    
    error_entry = {
        "timestamp": datetime.now(),
        "original_message": error_message,
        "category": error_category,
        "solutions": solutions or ["„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÁ¢∫Ë™ç„Åó„ÄÅÈÅ©Âàá„Å™‰øÆÊ≠£„ÇíË°å„Å£„Å¶„Åè„Å†„Åï„ÅÑ"]
    }
    
    st.session_state.error_history.append(error_entry)
    
    # Â±•Ê≠¥„ÅÆ‰∏äÈôêÂà∂Âæ°
    if len(st.session_state.error_history) > 50:
        st.session_state.error_history = st.session_state.error_history[-50:]

# =========================================================================
# „É¨„Ç¨„Ç∑„ÉºÈñ¢Êï∞„Çµ„Éù„Éº„ÉàÔºàanalysis_logic.py„Å®„ÅÆ‰∫íÊèõÊÄßÔºâ
# =========================================================================

def show_error_with_solutions(error: Exception, context: Dict[str, Any] = None):
    """„Ç®„É©„Éº„ÇíËß£Ê±∫Á≠ñ‰ªò„Åç„ÅßË°®Á§∫Ôºà„É¨„Ç¨„Ç∑„Éº‰∫íÊèõÔºâ"""
    # Êñ∞„Åó„ÅÑÂº∑Âåñ„Åï„Çå„Åü„Ç®„É©„ÉºË°®Á§∫„Çí‰ΩøÁî®
    show_enhanced_error_message(error, context)

# =========================================================================
# „Åù„ÅÆ‰ªñ„ÅÆ„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞
# =========================================================================

def get_error_summary() -> Dict[str, Any]:
    """„Ç®„É©„Éº„Çµ„Éû„É™„Éº„ÅÆÂèñÂæó"""
    handler = EnhancedErrorHandler()
    return handler.get_error_statistics()

def clear_error_history():
    """„Ç®„É©„ÉºÂ±•Ê≠¥„ÅÆ„ÇØ„É™„Ç¢"""
    if "error_history" in st.session_state:
        st.session_state.error_history = []
        st.success("‚úÖ „Ç®„É©„ÉºÂ±•Ê≠¥„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åó„Åü")

def export_error_log() -> str:
    """„Ç®„É©„Éº„É≠„Ç∞„ÅÆ„Ç®„ÇØ„Çπ„Éù„Éº„Éà"""
    if not st.session_state.get("error_history"):
        return json.dumps({"message": "„Ç®„É©„ÉºÂ±•Ê≠¥„Åå„ÅÇ„Çä„Åæ„Åõ„Çì"}, ensure_ascii=False, indent=2)
    
    export_data = {
        "export_timestamp": datetime.now().isoformat(),
        "total_errors": len(st.session_state.error_history),
        "errors": [
            {
                "timestamp": error.get("timestamp", datetime.now()).isoformat(),
                "category": error.get("category", "‰∏çÊòé"),
                "message": error.get("original_message", ""),
                "solutions": error.get("solutions", [])
            }
            for error in st.session_state.error_history
        ]
    }
    
    return json.dumps(export_data, ensure_ascii=False, indent=2, default=str)