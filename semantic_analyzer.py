# semantic_analyzer.py (æœ€çµ‚è§£æ±ºç‰ˆ)
import streamlit as st
from typing import List, Dict, Optional
import numpy as np
import pandas as pd

try:
    from bq_tool_config import settings
    SETTINGS_AVAILABLE = settings is not None
except ImportError:
    SETTINGS_AVAILABLE = False
    settings = None

# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# TextEmbeddingModel ã«ä¾å­˜ã—ãªã„ã€ã‚ˆã‚Šä½ãƒ¬ãƒ™ãƒ«ãªéƒ¨å“ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™
try:
    from google.cloud import aiplatform
    from google.protobuf import json_format
    from google.protobuf.struct_pb2 import Value
    AIPLATFORM_AVAILABLE = True
except ImportError:
    AIPLATFORM_AVAILABLE = False

@st.cache_data(show_spinner="AIãŒãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã„ã¾ã™...")
def generate_embeddings(texts: List[str]) -> Optional[Dict[str, List[float]]]:
    """
    TextEmbeddingModel ã‚’ä½¿ã‚ãšã«ã€Vertex AI Endpointã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    if not AIPLATFORM_AVAILABLE:
        st.error("âŒ `google-cloud-aiplatform`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    
    if SETTINGS_AVAILABLE:
        project_id = settings.bigquery.project_id
        location = settings.bigquery.location
    else:
        project_id = "vorn-digi-mktg-poc-635a"
        location = "asia-northeast1"

    if not project_id or not location:
        st.error("âŒ GCPã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã¨ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None

    try:
        # APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        client_options = {"api_endpoint": f"{location}-aiplatform.googleapis.com"}
        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)

        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        endpoint = f"projects/{project_id}/locations/{location}/publishers/google/models/text-embedding-004"
        
        # APIãŒè¦æ±‚ã™ã‚‹å½¢å¼ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        instances = [json_format.ParseDict({"content": text}, Value()) for text in texts]
        
        # APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        response = client.predict(endpoint=endpoint, instances=instances)
        
        # çµæœã‚’ã€Œãƒ†ã‚­ã‚¹ãƒˆ: ãƒ™ã‚¯ãƒˆãƒ«ã€ã®è¾æ›¸å½¢å¼ã«æ•´å½¢
        embedding_dict = {}
        for i, prediction in enumerate(response.predictions):
            vector = [v for v in prediction['embeddings']['values']]
            embedding_dict[texts[i]] = vector
            
        return embedding_dict

    except Exception as e:
        st.error(f"ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise e

def find_similar_texts(query_text: str, embeddings_dict: Dict[str, List[float]], top_n: int = 5) -> Optional[pd.DataFrame]:
    # (ã“ã®é–¢æ•°ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    if query_text not in embeddings_dict:
        st.error(f"åŸºæº–ãƒ†ã‚­ã‚¹ãƒˆ '{query_text}' ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    try:
        query_vector = np.array(embeddings_dict[query_text])
        texts = list(embeddings_dict.keys())
        vectors = np.array(list(embeddings_dict.values()))
        norm_query = query_vector / np.linalg.norm(query_vector)
        norm_vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
        similarities = np.dot(norm_vectors, norm_query)
        results_df = pd.DataFrame({"text": texts, "similarity": similarities})
        results_df = results_df[results_df["text"] != query_text]
        top_results = results_df.sort_values(by="similarity", ascending=False).head(top_n)
        return top_results
    except Exception as e:
        st.error(f"é¡ä¼¼æ€§è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

# semantic_analyzer.py ã«ä»¥ä¸‹ã‚’è¿½åŠ 

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import plotly.express as px

# (generate_embeddings, find_similar_texts ã¯æ—¢å­˜ã®ã¾ã¾)
# ...

def group_texts_by_meaning(texts: List[str], n_clusters: int = 5) -> Optional[pd.DataFrame]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’æ„å‘³ã«åŸºã¥ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

    Args:
        texts (List[str]): ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã€‚
        n_clusters (int): ä½œæˆã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°ã€‚

    Returns:
        Optional[pd.DataFrame]: ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¯ãƒ©ã‚¹ã‚¿ç•ªå·ã‚’å«ã‚€DataFrameã€‚
    """
    st.info(f"âš™ï¸ {len(texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
    embeddings_dict = generate_embeddings(texts)
    if not embeddings_dict:
        st.error("ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return None
    
    df = pd.DataFrame(embeddings_dict.items(), columns=['text', 'vector'])
    
    st.info(f"âš™ï¸ ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’{n_clusters}å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†é¡ã—ã¦ã„ã¾ã™...")
    vectors = np.array(df['vector'].tolist())
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df['cluster'] = kmeans.fit_predict(vectors)
    
    st.success("âœ… ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    return df[['text', 'cluster']]

def extract_tags_for_cluster(grouped_df: pd.DataFrame, model) -> Dict[int, List[str]]:
    """
    å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰è¤‡æ•°ã®ç‰¹å¾´çš„ãªã‚¿ã‚°ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    Args:
        grouped_df (pd.DataFrame): ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®DataFrameã€‚
        model: Geminiãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚

    Returns:
        Dict[int, List[str]]: ã‚¯ãƒ©ã‚¹ã‚¿ç•ªå·ã‚’ã‚­ãƒ¼ã€ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã€‚
    """
    if not model:
        st.warning("âš ï¸ AIãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚¿ã‚°æŠ½å‡ºã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return {}

    st.info("ğŸ¤– AIãŒå„ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´ã‚¿ã‚°ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
    cluster_tags = {}
    for cluster_id in sorted(grouped_df['cluster'].unique()):
        sample_texts = grouped_df[grouped_df['cluster'] == cluster_id]['text'].sample(min(10, len(grouped_df[grouped_df['cluster'] == cluster_id]))).tolist()

        texts_for_prompt = "\n- ".join(sample_texts)

        prompt = f"""
        # æŒ‡ç¤º
        ã‚ãªãŸã¯å„ªç§€ãªãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®åºƒå‘Šæ–‡ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€å…±é€šã™ã‚‹è¨´æ±‚ãƒã‚¤ãƒ³ãƒˆã‚„ç‰¹å¾´ã‚’åˆ†æã—ã€æœ€å¤§5å€‹ã®ã€Œ#ã€ã§å§‹ã¾ã‚‹ç°¡æ½”ãªã‚¿ã‚°ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        # åºƒå‘Šæ–‡ãƒªã‚¹ãƒˆ
        - {texts_for_prompt}

        # å‡ºåŠ›å½¢å¼ï¼ˆã“ã®å½¢å¼ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼‰
        - å¿…ãšã€Œ#ã€ã‹ã‚‰å§‹ã¾ã‚‹ã‚¿ã‚°ã®ã¿ã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        - è§£èª¬ã‚„æŒ¨æ‹¶ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚

        # å‡ºåŠ›ä¾‹
        #é€æ–™ç„¡æ–™
        #æœŸé–“é™å®šã‚»ãƒ¼ãƒ«
        #åˆå¿ƒè€…å‘ã‘
        """

        try:
            response = model.generate_content(prompt)
            # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡Œã”ã¨ã«åˆ†å‰²ã—ã€ç©ºè¡Œãªã©ã‚’é™¤å»
            tags = [tag.strip() for tag in response.text.strip().split('\n') if tag.strip()]
            cluster_tags[cluster_id] = tags
        except Exception as e:
            cluster_tags[cluster_id] = [f"ã‚¿ã‚°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}"]

    return cluster_tags

def reduce_dimensions_for_visualization(embeddings_dict: Dict[str, List[float]]) -> Optional[pd.DataFrame]:
    """
    ã€è¿½åŠ ææ¡ˆã€‘å¯è¦–åŒ–ã®ãŸã‚ã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’t-SNEã§2æ¬¡å…ƒã«å‰Šæ¸›ã™ã‚‹ã€‚

    Args:
        embeddings_dict (Dict[str, List[float]]): ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ™ã‚¯ãƒˆãƒ«ã®è¾æ›¸ã€‚

    Returns:
        Optional[pd.DataFrame]: ãƒ†ã‚­ã‚¹ãƒˆã¨2æ¬¡å…ƒåº§æ¨™ã‚’å«ã‚€DataFrameã€‚
    """
    if not embeddings_dict:
        return None
        
    texts = list(embeddings_dict.keys())
    vectors = np.array(list(embeddings_dict.values()))
    
    st.info("ğŸ¨ ã‚°ãƒ©ãƒ•è¡¨ç¤ºã®ãŸã‚ã«æ¬¡å…ƒå‰Šæ¸›ã‚’å®Ÿè¡Œä¸­...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(texts)-1))
    reduced_vectors = tsne.fit_transform(vectors)
    
    vis_df = pd.DataFrame(reduced_vectors, columns=['x', 'y'])
    vis_df['text'] = texts
    
    return vis_df