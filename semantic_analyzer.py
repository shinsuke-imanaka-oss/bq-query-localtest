# semantic_analyzer.py

import streamlit as st
from typing import List, Dict, Optional
import numpy as np
import pandas as pd
# â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒè¿½åŠ ãƒ»ä¿®æ­£ç®‡æ‰€ã§ã™ã€‘â–¼â–¼â–¼
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import plotly.express as px
import hdbscan
# â–²â–²â–²ã€è¿½åŠ ãƒ»ä¿®æ­£ã¯ã“ã“ã¾ã§ã§ã™ã€‘â–²â–²â–²

try:
    from bq_tool_config import settings
    SETTINGS_AVAILABLE = settings is not None
except ImportError:
    SETTINGS_AVAILABLE = False
    settings = None

# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from google.cloud import aiplatform
    from google.protobuf import json_format
    from google.protobuf.struct_pb2 import Value
    AIPLATFORM_AVAILABLE = True
except ImportError:
    AIPLATFORM_AVAILABLE = False

@st.cache_data(show_spinner="AIãŒãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã„ã¾ã™...")
def generate_embeddings(texts: List[str]) -> Optional[Dict[str, List[float]]]:
    """
    TextEmbeddingModel ã‚’ä½¿ã‚ãšã«ã€Vertex AI Endpointã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    if not AIPLATFORM_AVAILABLE:
        st.error("âŒ `google-cloud-aiplatform`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    if SETTINGS_AVAILABLE:
        project_id = settings.bigquery.project_id
        location = settings.bigquery.location
    else:
        project_id = "vorn-digi-mktg-poc-635a"
        location = "asia-northeast1"

    if not project_id or not location:
        st.error("âŒ GCPã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã¨ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None

    try:
        client_options = {"api_endpoint": f"{location}-aiplatform.googleapis.com"}
        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)
        endpoint = f"projects/{project_id}/locations/{location}/publishers/google/models/text-embedding-004"
        instances = [json_format.ParseDict({"content": text}, Value()) for text in texts]
        response = client.predict(endpoint=endpoint, instances=instances)
        embedding_dict = {}
        for i, prediction in enumerate(response.predictions):
            vector = [v for v in prediction['embeddings']['values']]
            embedding_dict[texts[i]] = vector
        return embedding_dict
    except Exception as e:
        st.error(f"ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise e

def find_similar_texts(query_text: str, embeddings_dict: Dict[str, List[float]], top_n: int = 5) -> Optional[pd.DataFrame]:
    if query_text not in embeddings_dict:
        st.error(f"åŸºæº–ãƒ†ã‚­ã‚¹ãƒˆ '{query_text}' ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    try:
        query_vector = np.array(embeddings_dict[query_text])
        texts = list(embeddings_dict.keys())
        vectors = np.array(list(embeddings_dict.values()))
        norm_query = query_vector / np.linalg.norm(query_vector)
        norm_vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
        similarities = np.dot(norm_vectors, norm_query)
        results_df = pd.DataFrame({"text": texts, "similarity": similarities})
        results_df = results_df[results_df["text"] != query_text]
        top_results = results_df.sort_values(by="similarity", ascending=False).head(top_n)
        return top_results
    except Exception as e:
        st.error(f"é¡ä¼¼æ€§è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def group_texts_by_meaning(texts: List[str], min_cluster_size: int = 3) -> Optional[pd.DataFrame]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’HDBSCANã‚’ç”¨ã„ã¦æ„å‘³ã«åŸºã¥ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
    """
    st.info(f"âš™ï¸ {len(texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
    embeddings_dict = generate_embeddings(texts)
    if not embeddings_dict:
        st.error("ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return None

    df = pd.DataFrame(embeddings_dict.items(), columns=['text', 'vector'])
    vectors = np.array(df['vector'].tolist())

    st.info(f"âš™ï¸ HDBSCANã§æœ€é©ãªã‚°ãƒ«ãƒ¼ãƒ—ã‚’è‡ªå‹•åˆ¤å®šã—ã¦ã„ã¾ã™ (æœ€å°ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º: {min_cluster_size})...")

    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=1,
        metric='euclidean',
        cluster_selection_method='eom'
    )
    df['cluster'] = clusterer.fit_predict(vectors)

    num_clusters_found = len(df[df['cluster'] != -1]['cluster'].unique())
    num_noise_points = len(df[df['cluster'] == -1])
    st.success(f"âœ… ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°å®Œäº†ï¼ {num_clusters_found}å€‹ã®ä¸»è¦ã‚°ãƒ«ãƒ¼ãƒ—ã¨{num_noise_points}å€‹ã®åˆ†é¡å¤–ãƒ†ã‚­ã‚¹ãƒˆãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸã€‚")

    return df[['text', 'cluster']]

def extract_tags_for_cluster(grouped_df: pd.DataFrame, model) -> Dict[int, List[str]]:
    """
    å„ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰è¤‡æ•°ã®ç‰¹å¾´çš„ãªã‚¿ã‚°ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    """
    if not model:
        st.warning("âš ï¸ AIãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚¿ã‚°æŠ½å‡ºã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return {}

    st.info("ğŸ¤– AIãŒå„ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´ã‚¿ã‚°ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
    cluster_tags = {}
    for cluster_id in sorted(grouped_df['cluster'].unique()):
        if cluster_id == -1: continue # ãƒã‚¤ã‚ºã¯ã‚¿ã‚°æŠ½å‡ºã®å¯¾è±¡å¤–
        sample_texts = grouped_df[grouped_df['cluster'] == cluster_id]['text'].sample(min(10, len(grouped_df[grouped_df['cluster'] == cluster_id]))).tolist()
        texts_for_prompt = "\n- ".join(sample_texts)
        prompt = f"""
        # æŒ‡ç¤º
        ã‚ãªãŸã¯å„ªç§€ãªãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
        ä»¥ä¸‹ã®åºƒå‘Šæ–‡ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€å…±é€šã™ã‚‹è¨´æ±‚ãƒã‚¤ãƒ³ãƒˆã‚„ç‰¹å¾´ã‚’åˆ†æã—ã€æœ€å¤§5å€‹ã®ã€Œ#ã€ã§å§‹ã¾ã‚‹ç°¡æ½”ãªã‚¿ã‚°ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        # åºƒå‘Šæ–‡ãƒªã‚¹ãƒˆ
        - {texts_for_prompt}
        # å‡ºåŠ›å½¢å¼ï¼ˆã“ã®å½¢å¼ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼‰
        - å¿…ãšã€Œ#ã€ã‹ã‚‰å§‹ã¾ã‚‹ã‚¿ã‚°ã®ã¿ã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        - è§£èª¬ã‚„æŒ¨æ‹¶ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚
        # å‡ºåŠ›ä¾‹
        #é€æ–™ç„¡æ–™
        #æœŸé–“é™å®šã‚»ãƒ¼ãƒ«
        #åˆå¿ƒè€…å‘ã‘
        """
        try:
            response = model.generate_content(prompt)
            tags = [tag.strip() for tag in response.text.strip().split('\n') if tag.strip()]
            cluster_tags[cluster_id] = tags
        except Exception as e:
            cluster_tags[cluster_id] = [f"ã‚¿ã‚°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}"]
    return cluster_tags

def reduce_dimensions_for_visualization(embeddings_dict: Dict[str, List[float]]) -> Optional[pd.DataFrame]:
    """
    å¯è¦–åŒ–ã®ãŸã‚ã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’t-SNEã§2æ¬¡å…ƒã«å‰Šæ¸›ã™ã‚‹ã€‚
    """
    if not embeddings_dict:
        return None

    texts = list(embeddings_dict.keys())
    vectors = np.array(list(embeddings_dict.values()))

    st.info("ğŸ¨ ã‚°ãƒ©ãƒ•è¡¨ç¤ºã®ãŸã‚ã«æ¬¡å…ƒå‰Šæ¸›ã‚’å®Ÿè¡Œä¸­...")
    # ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„å ´åˆã®perplexityã®èª¿æ•´
    perplexity_value = min(30, len(texts) - 1)
    if perplexity_value <= 0:
        st.warning("ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã™ãã‚‹ãŸã‚ã€å¯è¦–åŒ–ãƒãƒƒãƒ—ã¯ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚")
        return None

    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
    reduced_vectors = tsne.fit_transform(vectors)

    vis_df = pd.DataFrame(reduced_vectors, columns=['x', 'y'])
    vis_df['text'] = texts

    return vis_df